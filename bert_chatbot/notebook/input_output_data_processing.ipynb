{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../transformers'))\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from data_utils import CornellMovieDialogsProcessor, InputExample, InputFeatures, cornell_movie_convert_examples_to_features\n",
    "from utils import create_masks\n",
    "from transformers import BertTokenizer\n",
    "from bert_model import Config, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tain dataset length 177292\n",
      "\n",
      "test dataset length 44323\n",
      "177292\n"
     ]
    }
   ],
   "source": [
    "processor = CornellMovieDialogsProcessor()\n",
    "train_examples = processor.get_train_examples()\n",
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "we re late .\n",
      "fucking map quest . i never again . i b ext . downtown garage rooftop next b ziering and green approach a large tent . the winnebago is parked off to the side . alf is inside . . . doing something with a welder on the front of the rv .\n",
      "------------------------\n",
      "my client has ruled that out .\n",
      "my client is prepared to entertain an amicable dissolution of the marriage without prejudice .\n",
      "------------------------\n",
      "i ll be fine . . . are you a writer , mr fink ?\n",
      "yes i am . i m working on a wres please call me barton .\n",
      "------------------------\n",
      "i want to know what s going on . are you part of this ?\n",
      "what s going on ? i m going on my second job this month , and now i m going on unemployment .\n",
      "------------------------\n",
      "you might be the only one with a job .\n",
      "baby , don t talk like that . your rehab s going well . you ll be back before you know it .\n",
      "------------------------\n",
      "you wanna get me a fuckin snitch jacket ?\n",
      "you wanna buy your brother ten years . . . ? you don t have to say anything . just look at this list and point . here .\n",
      "------------------------\n",
      "i don t know . is that right ?\n",
      "and jewish men like to get it .\n",
      "------------------------\n",
      "the explosive override ?\n",
      "it s under water ! there s no way to reach it . . .\n",
      "------------------------\n",
      "is that the ransom file ? thanks . don t wear that outfit again .\n",
      "ummm . . . what ? i didn t hear you .\n",
      "------------------------\n",
      "you sure you want me to ?\n",
      "yeah , for some reason , i m sure . . .\n",
      "------------------------\n",
      "miles , i ve always been on the square with you .\n",
      "i m sure you have buzz .\n",
      "------------------------\n",
      "almost immediately .\n",
      "i d like to take everyone out after the show .\n"
     ]
    }
   ],
   "source": [
    "for (ex_index, example) in enumerate(train_examples):\n",
    "    #example = processor.get_example_from_tensor_dict(example)\n",
    "    print(\"------------------------\")\n",
    "    print(example.text_a)\n",
    "    print(example.text_b)\n",
    "    if ex_index > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "VOCAB_SIZE = 30522\n",
    "MAX_SEQ_LENGTH = 128\n",
    "config = Config(num_layers=6, d_model=256, dff=1024, num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "transformer = Transformer(config, VOCAB_SIZE, BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'special_tokens_mask': [1, 0, 0, 0, 0, 1], 'input_ids': [101, 2057, 2128, 2397, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'input_ids': [101, 8239, 4949, 8795, 1012, 1045, 2196, 2153, 1012, 1045, 1038, 4654, 2102, 1012, 5116, 7381, 23308, 2279, 1038, 1062, 21939, 3070, 1998, 2665, 3921, 1037, 2312, 9311, 1012, 1996, 2663, 2638, 16078, 2080, 2003, 9083, 2125, 2000, 1996, 2217, 1012, 24493, 2003, 2503, 1012, 1012, 1012, 2725, 2242, 2007, 1037, 2057, 16502, 2006, 1996, 2392, 1997, 1996, 27634, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "for (ex_index, example) in enumerate(train_examples):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "            example.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "        )\n",
    "\n",
    "    outputs = tokenizer.encode_plus(\n",
    "            example.text_b,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "        )\n",
    "\n",
    "    print(inputs)\n",
    "    print()\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (759 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_dataset = cornell_movie_convert_examples_to_features(train_examples, tokenizer, MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128)\n",
      "(32, 128)\n",
      "tf.Tensor(\n",
      "[ 101 2054 2079 2057 2079 1029  102    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(128,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[ 101 1045 2123 1056 2113 1012  102    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(128,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.shuffle(128).batch(BATCH_SIZE).repeat(-1)\n",
    "inputs, outputs = next(iter(train_dataset))\n",
    "print(inputs['input_ids'].shape)\n",
    "print(outputs['output_ids'].shape)\n",
    "print(inputs['input_ids'][0])\n",
    "print(outputs['output_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------inputs------------\n",
      "tf.Tensor(\n",
      "[ 101 2016 2351 1999 1037 2482 4926 1012  102    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(128,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(128,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(128,), dtype=int32)\n",
      "----------------outputs--------------\n",
      "(32, 128)\n",
      "tf.Tensor(\n",
      "[ 101 1045 1049 3374 1012  102    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(128,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset:\n",
    "    input_ids, input_attention_masks, input_token_type_ids = x[\"input_ids\"], x['attention_mask'], x['token_type_ids']\n",
    "    print(\"-----------------inputs------------\")\n",
    "    print(input_ids[0])\n",
    "    print(input_attention_masks[0])\n",
    "    print(input_token_type_ids[0])\n",
    "    output_ids = y[\"output_ids\"]\n",
    "    print(\"----------------outputs--------------\")\n",
    "    print(output_ids.shape)\n",
    "    print(output_ids[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 1, 128)\n",
      "(32, 1, 128, 128)\n",
      "(32, 1, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = next(iter(train_dataset))\n",
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inputs['input_ids'], outputs['output_ids'])\n",
    "print(enc_padding_mask.shape)\n",
    "print(combined_mask.shape)\n",
    "print(dec_padding_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "   1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "   1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "   1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "   1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "   1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]], shape=(1, 1, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(enc_padding_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 1. 1. ... 1. 1. 1.]\n",
      "  [0. 0. 1. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 1. 1. 1.]]], shape=(1, 128, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(combined_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 30522)\n"
     ]
    }
   ],
   "source": [
    "output,weights = transformer(inputs, outputs, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert_for_chat_bot_encoder (B multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  15707136  \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             multiple                  7844154   \n",
      "=================================================================\n",
      "Total params: 133,033,530\n",
      "Trainable params: 133,033,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    transformer, to_file='bert_chatbot_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
