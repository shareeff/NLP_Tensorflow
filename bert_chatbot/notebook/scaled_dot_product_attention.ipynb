{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../transformers'))\n",
    "sys.path.append(os.path.abspath('..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import create_masks, positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=256\n",
    "batch_size = 32\n",
    "seq_len = 128\n",
    "num_heads=8\n",
    "depth = d_model // num_heads\n",
    "vocab_size = 30522\n",
    "bert_d_model = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "#input = tf.random.uniform((batch_size, seq_len),maxval=vocab_size, dtype=tf.dtypes.int32)\n",
    "enc_output = tf.random.uniform((batch_size, seq_len, bert_d_model))\n",
    "dec_input = tf.random.uniform((batch_size, seq_len),maxval=vocab_size, dtype=tf.dtypes.int32)\n",
    "print(enc_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "pos_encoding = positional_encoding(vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 256)\n",
      "(32, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "x = embedding(dec_input)\n",
    "print(x.shape)\n",
    "x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "x += pos_encoding[:, :seq_len, :]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq = tf.keras.layers.Dense(d_model)\n",
    "wk = tf.keras.layers.Dense(d_model)\n",
    "wv = tf.keras.layers.Dense(d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q = wq(x) # (batch_size, seq_len, d_model)\n",
    "k = wk(x) # (batch_size, seq_len, d_model)\n",
    "v = wv(x) # (batch_size, seq_len, d_model)\n",
    "print(\"shape q: {}, k: {}, v: {}\".format(q.shape,k.shape,v.shape))\n",
    "# shape q: (32, 128, 256), k: (32, 128, 256), v: (32, 128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape q: (32, 128, 256), k: (32, 128, 256), v: (32, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "q = wq(x) # (batch_size, seq_len, d_model)\n",
    "k = wk(enc_output) # (batch_size, seq_len, d_model)\n",
    "v = wv(enc_output) # (batch_size, seq_len, d_model)\n",
    "print(\"shape q: {}, k: {}, v: {}\".format(q.shape,k.shape,v.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "k = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "v = tf.random.uniform((batch_size, seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, num_heads, depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 128, 32)\n",
      "(32, 8, 128, 32)\n",
      "(32, 8, 128, 32)\n"
     ]
    }
   ],
   "source": [
    "q = split_heads(q) # (batch_size, num_heads, seq_len_q, depth)\n",
    "k = split_heads(k) # (batch_size, num_heads, seq_len_q, depth)\n",
    "v = split_heads(v) # (batch_size, num_heads, seq_len_q, depth)\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
    "print(matmul_qk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "print(dk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_attention_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask: enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 1, 128)\n",
      "(32, 1, 128, 128)\n",
      "(32, 1, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "demo_input = tf.random.uniform((batch_size, seq_len),maxval=vocab_size, dtype=tf.dtypes.int32)\n",
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(demo_input, dec_input)\n",
    "print(enc_padding_mask.shape)\n",
    "print(combined_mask.shape)\n",
    "print(dec_padding_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about combine or look ahead mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "look_ahead_mask = create_look_ahead_mask(10)\n",
    "print(look_ahead_mask.shape)\n",
    "print(look_ahead_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look_ahead_mask sequentially view one word at a time like LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add mask to the scaled_attention_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "logits1 = scaled_attention_logits + (dec_padding_mask * -1e9)\n",
    "print(logits1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "logits2 = scaled_attention_logits + (combined_mask * -1e9)\n",
    "print(logits2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
